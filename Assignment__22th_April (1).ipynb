{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "gqBxOVsn6YPA"
      },
      "outputs": [],
      "source": [
        "# Task1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#: Read given data into DataFrame in python “Iris.csv”. Perform Data\n",
        "#cleaning.\n"
      ],
      "metadata": {
        "id": "Rw3kNVgR69ky"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming the file is downloaded or accessible locally\n",
        "file_path = \"/content/Iris.csv\"  # Replace with the actual path if needed\n",
        "\n",
        "# Read the data into a DataFrame\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Check for missing values\n",
        "missing_values_count = df.isnull().sum()\n",
        "print(missing_values_count)  # If there are missing values, handle them appropriately\n",
        "\n",
        "# Explore the data (optional)\n",
        "print(df.head())  # Display the first few rows\n",
        "print(df.describe())  # Get summary statistics (mean, std, quartiles, etc.)\n",
        "print(df.info())  # View data types and non-null values\n",
        "\n",
        "# Example of data cleaning (if needed)\n",
        "# For example, if there are categorical features with typos or inconsistencies, you might:\n",
        "# df['column_name'] = df['column_name'].str.lower()  # Convert to lowercase\n",
        "# df['column_name'] = df['column_name'].replace({'typo1': 'corrected_value', 'typo2': 'corrected_value'})  # Replace typos\n",
        "\n",
        "# Proceed with your data analysis or modeling tasks using the cleaned DataFrame 'df'\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S86J51NR7Xzw",
        "outputId": "4e235a0c-0ccc-4e56-97ba-c557d0b8fd9d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Id               0\n",
            "SepalLengthCm    0\n",
            "SepalWidthCm     0\n",
            "PetalLengthCm    0\n",
            "PetalWidthCm     0\n",
            "Species          0\n",
            "dtype: int64\n",
            "   Id  SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm      Species\n",
            "0   1            5.1           3.5            1.4           0.2  Iris-setosa\n",
            "1   2            4.9           3.0            1.4           0.2  Iris-setosa\n",
            "2   3            4.7           3.2            1.3           0.2  Iris-setosa\n",
            "3   4            4.6           3.1            1.5           0.2  Iris-setosa\n",
            "4   5            5.0           3.6            1.4           0.2  Iris-setosa\n",
            "               Id  SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm\n",
            "count  150.000000     150.000000    150.000000     150.000000    150.000000\n",
            "mean    75.500000       5.843333      3.054000       3.758667      1.198667\n",
            "std     43.445368       0.828066      0.433594       1.764420      0.763161\n",
            "min      1.000000       4.300000      2.000000       1.000000      0.100000\n",
            "25%     38.250000       5.100000      2.800000       1.600000      0.300000\n",
            "50%     75.500000       5.800000      3.000000       4.350000      1.300000\n",
            "75%    112.750000       6.400000      3.300000       5.100000      1.800000\n",
            "max    150.000000       7.900000      4.400000       6.900000      2.500000\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 150 entries, 0 to 149\n",
            "Data columns (total 6 columns):\n",
            " #   Column         Non-Null Count  Dtype  \n",
            "---  ------         --------------  -----  \n",
            " 0   Id             150 non-null    int64  \n",
            " 1   SepalLengthCm  150 non-null    float64\n",
            " 2   SepalWidthCm   150 non-null    float64\n",
            " 3   PetalLengthCm  150 non-null    float64\n",
            " 4   PetalWidthCm   150 non-null    float64\n",
            " 5   Species        150 non-null    object \n",
            "dtypes: float64(4), int64(1), object(1)\n",
            "memory usage: 7.2+ KB\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 2"
      ],
      "metadata": {
        "id": "PELyIRAn9Urs"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# After data cleaning, you are required to prepare your dataset for training.  • Separate features and labels.\n",
        "#• Vectorization\n",
        "#• Feature scaling/Normalisation\n",
        "#• Perform Label Encoding\n",
        "#• Split dataset into training and testing data"
      ],
      "metadata": {
        "id": "O-TI8FUj9bZ1"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "LGTy0q5_D315"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming the file is uploaded to Google Colab environment\n",
        "file_path = \"/content/Iris.csv\"  # Make sure to upload the file first"
      ],
      "metadata": {
        "id": "T60-j1sGD34F"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the data into a DataFrame\n",
        "df = pd.read_csv(file_path)"
      ],
      "metadata": {
        "id": "mA-JR7PhD36p"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming the file is downloaded or accessible locally (replace with your actual path)\n",
        "file_path = \"/content/Iris.csv\"\n",
        "\n",
        "# Read the data into a DataFrame\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Check if 'species' column exists before dropping\n",
        "if 'species' in df.columns:\n",
        "  # Separate features and labels (assuming 'species' is the target column)\n",
        "  features = df.drop('species', axis=1)  # Select all columns except 'species'\n",
        "  labels = df['species']\n",
        "  # Now you have features and labels in separate DataFrames\n",
        "  print(\"Features:\\n\", features)\n",
        "  print(\"Labels:\\n\", labels)\n",
        "else:\n",
        "  print(\"The 'species' column is not found in the DataFrame.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JxAfktRuD4Jd",
        "outputId": "8351437a-d2bb-48cf-8893-c3367e2da433"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The 'species' column is not found in the DataFrame.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load the iris dataset\n",
        "iris = load_iris()\n",
        "\n",
        "# Separate features and labels\n",
        "X = iris.data  # Features\n",
        "y = iris.target  # Labels\n",
        "\n",
        "# Print the shapes of features and labels\n",
        "print(\"Features shape:\", X.shape)\n",
        "print(\"Labels shape:\", y.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HNCCFLDH-5Av",
        "outputId": "97600148-77b5-43a6-979f-11121ed127f2"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Features shape: (150, 4)\n",
            "Labels shape: (150,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#• Vectorization"
      ],
      "metadata": {
        "id": "PdpCNgj3-5C5"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Context\tMeaning\tExample\n",
        "#Image Processing\tConverting raster images to vector graphics\tConverting a pixelated photo into a scalable logo\n",
        "#Machine Learning\tConverting data into numerical representation\tEncoding color names (blue, green, red) into numbers (1, 2, 3)\n",
        "\n",
        "#pen_spark\n",
        "\n",
        "#drive_spreadsheet\n",
        "#Export to Sheets\n",
        "#"
      ],
      "metadata": {
        "id": "BkkqDWfw-5Ga"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Feature Scaling/Normalization:"
      ],
      "metadata": {
        "id": "u2OFRhBG-5JH"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Standardization:\n",
        "\n",
        "# Scales features by subtracting the mean and dividing by the standard deviation."
      ],
      "metadata": {
        "id": "nMcOHm8w-5Ls"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)  # X is your feature matrix\n"
      ],
      "metadata": {
        "id": "d0Nkkuy--5Pa"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Label Encoding:\n",
        "\n",
        "# Label encoding is a technique for converting categorical labels (text labels like \"blue,\" \"green\")\n",
        "#  into numerical values. While it seems straightforward, be aware that this encoding can introduce an order where there might not be one inherently\n",
        "#  (e.g., \"1\" for blue might be seen as \"better\" than \"2\" for green). Here's an example:"
      ],
      "metadata": {
        "id": "AqBu2n6T-5S5"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "encoder = LabelEncoder()\n",
        "y_encoded = encoder.fit_transform(y)  # y is your label vector\n"
      ],
      "metadata": {
        "id": "T_kWKs21-5Wa"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting Dataset into Training and Testing Data:\n",
        "\n",
        "# Splitting your data into training and testing sets is crucial\n",
        "\n",
        "# for evaluating your machine learning model's performance.\n",
        "# The training set is used to train the model, and\n",
        "# the testing set is used to assess its generalizability on unseen data.\n",
        "# Here's an example using scikit-learn's train_test_split function:"
      ],
      "metadata": {
        "id": "aTw3gXEr-5ZN"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_encoded, test_size=0.2, random_state=42)  # X_scaled is your scaled feature matrix, y_encoded is your encoded label vector\n"
      ],
      "metadata": {
        "id": "n7wNEEIN-5dn"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Putting it all together:\n",
        "\n",
        "# Here's an example workflow that incorporates these steps"
      ],
      "metadata": {
        "id": "uah-E-7ACC_s"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# ... (load your data into X and y)\n",
        "\n",
        "# Feature scaling (example using MinMaxScaler)\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Label encoding\n",
        "encoder = LabelEncoder()\n",
        "y_encoded = encoder.fit_transform(y)\n",
        "\n",
        "# Splitting data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_encoded, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "BoqmmZ25C_ct"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform all types of SVM classifications on the above prepared Dataset."
      ],
      "metadata": {
        "id": "NmWHBFKjC_e0"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import cross_val_predict, confusion_matrix\n",
        "\n",
        "# Define different kernel functions for SVM\n",
        "kernel_types = ['linear', 'rbf', 'sigmoid']\n",
        "\n",
        "for kernel in kernel_types:\n",
        "  # Create an SVM model with the specified kernel\n",
        "  model = SVC(kernel=kernel)\n",
        "\n",
        "  # Train the model on the training data\n",
        "  model.fit(X_train, y_train)\n",
        "\n",
        "  # Make predictions on the testing set\n",
        "  y_pred = model.predict(X_test)\n",
        "\n",
        "  # Print classification report\n",
        "  print(f\"Classification Report for SVM with {kernel} kernel:\")\n",
        "  print(classification_report(y_test, y_pred))\n",
        "\n",
        "  # Perform cross-validation and get confusion matrix\n",
        "  y_train_pred = cross_val_predict(model, X_train, y_train)\n",
        "  conf_mx = confusion_matrix(y_train, y_train_pred)\n",
        "\n",
        "  print(f\"Confusion Matrix for SVM with {kernel} kernel:\")\n",
        "  print(conf_mx)\n",
        "\n",
        "  # Print a line for separation between kernel types\n",
        "  print(\"-\" * 60)\n"
      ],
      "metadata": {
        "id": "zF4EnBWJC_1f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Display confusion matrix and generate report of f1-score, recall and precision."
      ],
      "metadata": {
        "id": "6zCuAXj3FEs1"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  # Make predictions on the testing set\n",
        "  y_pred = model.predict(X_test)\n",
        "\n",
        "  # Print classification report\n",
        "  print(f\"Classification Report for SVM with {kernel} kernel:\")\n",
        "  print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "3ApJHsMSFM6t"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}